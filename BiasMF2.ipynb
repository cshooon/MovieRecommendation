{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "mount_file_id": "1QQlyO9Nb2ghzKFEaMGorui4FK2et7VIN",
      "authorship_tag": "ABX9TyOIH0i3ZuTMPklDGq+mBq64",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cshooon/MovieRecommendation/blob/main/BiasMF2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnha0vkrpWWh",
        "outputId": "d6afe8ba-e762-433a-97db-f4e4f9a69d1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vq9S2V2LpQ50"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_auc_score, mean_squared_error\n",
        "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "from torch.utils.data import DataLoader, BatchSampler, SequentialSampler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MovieLens(Dataset):\n",
        "    def __init__(self, user_tensor, item_tensor, target_tensor):\n",
        "        self.user_tensor = user_tensor\n",
        "        self.item_tensor = item_tensor\n",
        "        self.target_tensor = target_tensor\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.user_tensor[index], self.item_tensor[index], self.target_tensor[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.user_tensor.size(0)\n",
        "\n",
        "def load_and_sample_data(csv_file, frac=0.1):\n",
        "    df = pd.read_csv(csv_file)\n",
        "    df = df.sample(frac=frac, random_state=42)\n",
        "    return df\n",
        "\n",
        "ratings_path = '/content/drive/MyDrive/data/ratings.csv'\n",
        "movies_path = '/content/drive/MyDrive/data/movies.csv'\n",
        "\n",
        "ratings = load_and_sample_data(ratings_path)\n",
        "movies = load_and_sample_data(movies_path)\n",
        "\n",
        "# 영화 정보와 평점 정보를 결합\n",
        "full_data = pd.merge(ratings, movies, on='movieId', how='left')\n",
        "\n",
        "# 타임스탬프를 기준으로 데이터를 분할하여 트레이닝, 검증, 테스트 세트를 생성\n",
        "full_data_sorted = full_data.sort_values('timestamp')\n",
        "train_cutoff = int(0.7 * len(full_data_sorted))\n",
        "val_cutoff = int(0.85 * len(full_data_sorted))\n",
        "\n",
        "train_df = full_data_sorted[:train_cutoff].copy()\n",
        "validation_df = full_data_sorted[train_cutoff:val_cutoff].copy()\n",
        "test_df = full_data_sorted[val_cutoff:].copy()\n",
        "\n",
        "# 사용자와 아이템의 인덱스를 생성\n",
        "user_to_index = {user_id: index for index, user_id in enumerate(sorted(full_data['userId'].unique()))}\n",
        "item_to_index = {item_id: index for index, item_id in enumerate(sorted(full_data['movieId'].unique()))}\n",
        "\n",
        "def create_dataset(df, user_to_index, item_to_index):\n",
        "    df_copy = df.copy()\n",
        "    df_copy.loc[:, 'userIndex'] = df['userId'].map(user_to_index)\n",
        "    df_copy.loc[:, 'itemIndex'] = df['movieId'].map(item_to_index)\n",
        "\n",
        "    user_tensor = torch.tensor(df_copy['userIndex'].values, dtype=torch.long)\n",
        "    item_tensor = torch.tensor(df_copy['itemIndex'].values, dtype=torch.long)\n",
        "    target_tensor = torch.tensor(df_copy['rating'].values, dtype=torch.float32)\n",
        "\n",
        "    return MovieLens(user_tensor, item_tensor, target_tensor)\n",
        "\n",
        "def filter_users(df, min_ratings=10):\n",
        "    # 각 userId별 평가 항목 수 계산\n",
        "    user_rating_counts = df['userId'].value_counts()\n",
        "\n",
        "    # 10개 이상의 평가를 한 userId만 필터링\n",
        "    valid_users = user_rating_counts[user_rating_counts >= min_ratings].index\n",
        "\n",
        "    # 해당 userId의 데이터만 남김\n",
        "    return df[df['userId'].isin(valid_users)]\n",
        "\n",
        "# 필터링 적용\n",
        "validation_df = filter_users(validation_df)\n",
        "test_df = filter_users(test_df)\n",
        "\n",
        "validation_df = validation_df.sort_values(by='userId')\n",
        "test_df = test_df.sort_values(by='userId')\n",
        "\n",
        "train_dataset = create_dataset(train_df, user_to_index, item_to_index)\n",
        "validation_dataset = create_dataset(validation_df, user_to_index, item_to_index)\n",
        "test_dataset = create_dataset(test_df, user_to_index, item_to_index)\n",
        "\n",
        "batch_size = 256\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "tVLpOTtK0Xk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UserBatchSampler(BatchSampler):\n",
        "    def __init__(self, data_source):\n",
        "        self.user_batches = []\n",
        "        self.data_source = data_source\n",
        "        self.create_batches()\n",
        "\n",
        "    def create_batches(self):\n",
        "        user_data = {}\n",
        "        for idx, (user_id, _, _) in enumerate(self.data_source):\n",
        "            user_id = user_id.item()\n",
        "            if user_id not in user_data:\n",
        "                user_data[user_id] = []\n",
        "            user_data[user_id].append(idx)\n",
        "\n",
        "        self.user_batches = list(user_data.values())\n",
        "\n",
        "    def __iter__(self):\n",
        "        for batch in self.user_batches:\n",
        "            yield batch\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.user_batches)\n",
        "\n",
        "# 사용자별 BatchSampler 사용\n",
        "val_batch_sampler = UserBatchSampler(validation_dataset)\n",
        "test_batch_sampler = UserBatchSampler(test_dataset)\n",
        "\n",
        "val_loader = DataLoader(validation_dataset, batch_sampler=val_batch_sampler)\n",
        "test_loader = DataLoader(test_dataset, batch_sampler=test_batch_sampler)"
      ],
      "metadata": {
        "id": "ZnSQi_OZCh2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/AmazingDD/MF-pytorch/blob/master/BiasMFRecommender.py\n",
        "\n",
        "class BiasMF(torch.nn.Module):\n",
        "    def __init__(self, params):\n",
        "        super(BiasMF, self).__init__()\n",
        "        self.num_users = params['num_users']\n",
        "        self.num_items = params['num_items']\n",
        "        self.latent_dim = params['latent_dim']\n",
        "        self.mu = params['global_mean']\n",
        "\n",
        "        self.user_embedding = torch.nn.Embedding(self.num_users, self.latent_dim)\n",
        "        self.item_embedding = torch.nn.Embedding(self.num_items, self.latent_dim)\n",
        "\n",
        "        self.user_bias = torch.nn.Embedding(self.num_users, 1)\n",
        "        self.user_bias.weight.data = torch.zeros(self.num_users, 1).float()\n",
        "        self.item_bias = torch.nn.Embedding(self.num_items, 1)\n",
        "        self.item_bias.weight.data = torch.zeros(self.num_items, 1).float()\n",
        "\n",
        "    def forward(self, user_indices, item_indices):\n",
        "        user_vec = self.user_embedding(user_indices)\n",
        "        item_vec = self.item_embedding(item_indices)\n",
        "        dot = torch.mul(user_vec, item_vec).sum(dim=1)\n",
        "\n",
        "        rating = dot + self.mu + self.user_bias(user_indices).view(-1) + self.item_bias(item_indices).view(-1) + self.mu\n",
        "\n",
        "        return rating"
      ],
      "metadata": {
        "id": "nyxHDvaoSSzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 매개변수 설정\n",
        "params = {\n",
        "    'num_users': len(user_to_index),\n",
        "    'num_items': len(item_to_index),\n",
        "    'latent_dim': 50,\n",
        "    'global_mean': 3.0\n",
        "}\n",
        "\n",
        "model = BiasMF(params)\n",
        "loss_function = torch.nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "# GPU 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22fiKJRCdWWB",
        "outputId": "1618f421-559e-4d0b-be8a-68457e65d999"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BiasMF(\n",
              "  (user_embedding): Embedding(159453, 50)\n",
              "  (item_embedding): Embedding(31805, 50)\n",
              "  (user_bias): Embedding(159453, 1)\n",
              "  (item_bias): Embedding(31805, 1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/guoyang9/NCF/blob/master/evaluate.py\n",
        "def hit(gt_item, pred_items):\n",
        "\tif gt_item in pred_items:\n",
        "\t\treturn 1\n",
        "\treturn 0\n",
        "\n",
        "\n",
        "def ndcg(gt_item, pred_items):\n",
        "\tif gt_item in pred_items:\n",
        "\t\tindex = pred_items.index(gt_item)\n",
        "\t\treturn np.reciprocal(np.log2(index+2))\n",
        "\treturn 0"
      ],
      "metadata": {
        "id": "L_q2QjeMmF8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, loader, loss_function, top_k, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    HR, NDCG = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for user, item, label in tqdm(loader):\n",
        "            user, item, label = user.to(device), item.to(device), label.to(device)\n",
        "            predictions = model(user, item)\n",
        "            loss = loss_function(predictions, label)\n",
        "            total_loss += loss.item()\n",
        "            _, indices = torch.topk(predictions, top_k)\n",
        "\n",
        "            recommends = torch.take(item, indices).cpu().numpy().tolist()\n",
        "\n",
        "            # 각 사용자별 실제 상위 평점 아이템 추출\n",
        "            _, label_indices = torch.topk(label, top_k)\n",
        "            gt_items_batch = torch.take(item, label_indices).cpu().numpy()\n",
        "\n",
        "            for gt_items in gt_items_batch:\n",
        "                HR.append(hit(gt_items, recommends))\n",
        "                NDCG.append(ndcg(gt_items, recommends))\n",
        "\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    return avg_loss, np.mean(HR), np.mean(NDCG)"
      ],
      "metadata": {
        "id": "0m7QwBjSY3T5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 5\n",
        "\n",
        "# 훈련 및 검증 루프\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for user_indices, item_indices, ratings in tqdm(train_loader):\n",
        "        user_indices, item_indices, ratings = user_indices.to(device), item_indices.to(device), ratings.to(device)\n",
        "\n",
        "        # 예측 및 손실 계산\n",
        "        predictions = model(user_indices, item_indices)\n",
        "        loss = loss_function(predictions, ratings)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # 역전파 및 최적화\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    avg_val_loss, HR, NDCG = evaluate_model(model, val_loader, loss_function, 10, device)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_train_loss:.4f}\")\n",
        "    print(f\"Validation Loss: {avg_val_loss:.4f}, HR: {HR:.4f}, NDCG: {NDCG:.4f}\")\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "SGzvzTqFXwlF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fc246db-2dd9-4a0c-e2bc-4c39621af67b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6836/6836 [00:31<00:00, 215.62it/s]\n",
            "100%|██████████| 10573/10573 [00:11<00:00, 909.30it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Training Loss: 41.3508\n",
            "Validation Loss: 36.6970, HR: 0.5360, NDCG: 0.2424\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6836/6836 [00:31<00:00, 214.92it/s]\n",
            "100%|██████████| 10573/10573 [00:11<00:00, 891.26it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5, Training Loss: 17.0495\n",
            "Validation Loss: 25.5352, HR: 0.5336, NDCG: 0.2409\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6836/6836 [00:31<00:00, 215.73it/s]\n",
            "100%|██████████| 10573/10573 [00:10<00:00, 992.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5, Training Loss: 7.4887\n",
            "Validation Loss: 20.1674, HR: 0.5361, NDCG: 0.2416\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6836/6836 [00:31<00:00, 219.99it/s]\n",
            "100%|██████████| 10573/10573 [00:11<00:00, 913.30it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5, Training Loss: 3.7281\n",
            "Validation Loss: 17.3782, HR: 0.5403, NDCG: 0.2433\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6836/6836 [00:31<00:00, 216.39it/s]\n",
            "100%|██████████| 10573/10573 [00:11<00:00, 897.06it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5, Training Loss: 2.1090\n",
            "Validation Loss: 15.7912, HR: 0.5436, NDCG: 0.2446\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.save(model.state_dict(), '/content/drive/MyDrive/data/MF_model.pth')\n",
        "avg_val_loss, HR, NDCG = evaluate_model(model, test_loader, loss_function, 10, device)\n",
        "print(f\"Test Loss: {avg_val_loss:.4f}, HR: {HR:.4f}, NDCG: {NDCG:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rF9bpwfHTntk",
        "outputId": "cea82abd-2a09-4539-ec7e-d1987adf6f22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10150/10150 [00:11<00:00, 877.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 20.5621, HR: 0.5188, NDCG: 0.2338\n"
          ]
        }
      ]
    }
  ]
}